{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished.\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "import numpy\n",
    "import theano\n",
    "\n",
    "import theano.tensor as T\n",
    "\n",
    "import ipdb\n",
    "\n",
    "MINI_BATCH_SIZE = 500\n",
    "            \n",
    "# This function will extract the images from the\n",
    "# images classes, and place that information into\n",
    "# two separate shared theano variables.\n",
    "#\n",
    "# We use shared variables because When storing data \n",
    "# on the GPU it has to be stored as floats\n",
    "# therefore we will store the labels as ‘‘floatX‘‘ \n",
    "# as well (‘‘shared_y‘‘ does exactly that). But \n",
    "# during our computations we need them as ints \n",
    "# (we use labels as index, and if they are\n",
    "# floats it doesn’t make sense) therefore \n",
    "# instead of returning ‘‘shared_y‘‘ we will have \n",
    "# to cast it to int. This little hack lets us \n",
    "# get around this issue.\n",
    "#\n",
    "# You can tune the size of the mini-batches to fit\n",
    "# in available GPU memory.\n",
    "def shared_dataset(data_xy):\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(\n",
    "        numpy.asarray(\n",
    "            data_x, \n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    shared_y = theano.shared(\n",
    "        numpy.asarray(\n",
    "            data_y, \n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "\n",
    "# Opening the zipped file and extracting the contents.\n",
    "f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "\n",
    "# Loading the pickled data from the compressed file.\n",
    "# The file is formated as a 3-tuple of tuples,\n",
    "# where each tuple consists of a list of 10000 \n",
    "# data elements (the normalized images) and \n",
    "# 10000 class labels (a number, 0-9, that\n",
    "# indicates the class of the associated data element).\n",
    "#\n",
    "# Each data element is a normalized 28x28 MNIST \n",
    "# image rendered in a single list of 784 elements \n",
    "# (e.g. 28 x 28).\n",
    "#\n",
    "# So, to extract the image from the train_set tuple, \n",
    "# you'd use something like: \n",
    "# \n",
    "# images = train_set[0]\n",
    "# classes = train_set[1]\n",
    "# first_image = images[0]\n",
    "# first_image_class = classes[0]\n",
    "#\n",
    "# Loading the tuples from the 3-tuple file format,\n",
    "# and closing the archive. This could be done with\n",
    "# the 'with' statement as well.\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Create the extraced data.\n",
    "test_set_x, test_set_y = shared_dataset(test_set)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "# Accessing the first three minibatch of the training\n",
    "# set. You would generally extract the data in this way,\n",
    "# processing data as mini-batches of MINI_BATCH_SIZE.\n",
    "data_1 = train_set_x[0 * MINI_BATCH_SIZE : 1 * MINI_BATCH_SIZE]\n",
    "data_2 = train_set_x[1 * MINI_BATCH_SIZE : 2 * MINI_BATCH_SIZE]\n",
    "data_3 = train_set_x[2 * MINI_BATCH_SIZE : 3 * MINI_BATCH_SIZE]\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, datapoints_dim, label_dim):\n",
    "        # Here, we initialize an empty array with the dimentions\n",
    "        # of the number of classes by the number of elements.\n",
    "        #\n",
    "        # In our MNIST example, this will yield a 768 by 10\n",
    "        # two-dimensional shared numpy array.\n",
    "        #\n",
    "        # Named 'W' as it's the weights array. This is a 768x10\n",
    "        # array as we will have a sequence of 768 possible connections\n",
    "        # to each output node, and there's 10 output nodes. An\n",
    "        # input value will have 768 elements.\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (datapoints_dim, label_dim),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        # These are bias values, and we'll use them later.\n",
    "        # In our MNIST example, we will have one bias per class.\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (label_dim,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        # An MNIST input value has 768 elements, so we can matrix\n",
    "        # multiply with W (see T.dot(.) below). This yields a 10\n",
    "        # element vector we can then add to the bias values (e.g. self.b).\n",
    "        # Then, we use softmax to determine the distribution.\n",
    "        #\n",
    "        # As we take these values to be the probabilities that the submitted\n",
    "        # input belongs to a given class, we call the resulting 10 element\n",
    "        # vector the p_y_given_x (the probability that the input is a member\n",
    "        # of class Y[i] given the probabilities in W biased by b).\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "        \n",
    "        # This is the predicted class of the submitted input - essentially\n",
    "        # the most likely class based on the probabilities obtained above.\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        \n",
    "        # This is essentially the model. This would usually be saved.\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def negative_log_likelihood(self, y):\n",
    "        return T.mean(\n",
    "            T.log(self.p_y_given_x)[T.arange(y.shape[0]), y]\n",
    "        )\n",
    "    \n",
    "    def errors(self, y):\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "# Working with Logistic Regression\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "classifier = LogisticRegression(input=x, datapoints_dim=28*28, label_dim=10)\n",
    "cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "print 'finished.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
